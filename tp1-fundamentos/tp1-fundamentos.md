# Ejercicio 1:

## Weak IA: CAN MACHINES ACT INTELLIGENTLY?

La inteligencia artificial debil es la afirmacion de que las computadoras pueden emular la inteligencia. Al principio se afirmaba que "cada aspecto del aprendizaje o cualquier otra característica de la inteligencia puede describirse con tanta precisión que se puede hacer que una máquina lo simule". Por lo tanto, la IA se fundó sobre la suposición de que es posible una IA débil. Otros han afirmado que una IA débil es imposible: "La inteligencia artificial perseguida dentro del culto del computacionalismo no tiene ni siquiera la menor posibilidad de producir resultados duraderos"

La imposibilidad de la IA depende de cómo se defina. Como definimos AI como la búsqueda del mejor programa de agente en una arquitectura dada , para cualquier arquitectura digital con k bits de almacenamiento de programas hay exactamente 2k programas de agentes, y todo lo que tenemos que hacer para encontrar el mejor es enumerarlos y probarlos todos. . Con esta formulación, la IA es posible por definición. Solo que no sería factible para k grande.

Pero a los filósofos les interesa el problema de comparar dos arquitecturas, la humana y la de la máquina. Además, tradicionalmente han planteado la pregunta no en términos de maximizar la utilidad esperada, sino más bien como: "¿Pueden pensar las máquinas?"

Alan Turing, en su famoso artículo "Computing Machinery and Intelligence" (1950), sugirió que en lugar de preguntar si las máquinas pueden pensar, deberíamos preguntarnos si las máquinas pueden pasar una prueba de inteligencia conductual, que ha llegado a llamarse la prueba de Turing. 

Turing conjeturó que, para el año 2000, un ordenador con un almacenamiento de 109 unidades podría programarse lo suficientemente bien como para pasar la prueba. Se equivocó: los programas aún no han engañado a un juez sofisticado.

Con el paso del tiempo fueron apareciendo argumentos en contra de las maquinas inteligentes, uno de estos es el "argumento de la incapacidad" afirma que "una máquina nunca puede hacer X". Como ejemplos de X, Turing enumera los siguientes: Ser amable, ingenioso, hermoso, amigable, tener iniciativa, tener sentido del humor, distinguir el bien del mal, cometer errores, enamorarse, entre otros.

Otro argumento es "La objeción matemática", se conoce que ciertas preguntas matemáticas son, en principio, incontestables por sistemas formales particulares. El teorema de completitud de G ̈odel (ver Sección 9.5) es el ejemplo más famoso de esto. Brevemente, para cualquier sistema axiomático formal F lo suficientemente potente como para hacer aritmética, es posible construir una oración llamada Godel G(F) con las siguientes propiedades: • G(F) es una oración de F, pero no puede ser probada dentro de F. • Si F es consistente, entonces G(F) es verdadera.

Filósofos como J. R. Lucas (1961) han afirmado que este teorema muestra que las máquinas son mentalmente inferiores a los humanos, porque las máquinas son sistemas formales que están limitados por el teorema de incompletitud no pueden establecer la verdad de su propia oración Gödel—, mientras que los humanos no tienen tal limitación. 

El proximo argumento presentado es el "Argumento de la informalidad del comportamiento". Esencialmente, esta es la afirmación de que el comportamiento humano es demasiado complejo para ser capturado por un conjunto simple de reglas y que debido a que los ordenadores no pueden hacer más que seguir un conjunto de reglas, no pueden generar un comportamiento tan inteligente como el de los humanos.

Dreyfus y Dreyfus proponen un proceso de cinco etapas para adquirir experiencia, comenzando con el procesamiento basado en reglas y terminando con la capacidad de responder instantáneamente. A pesar de ser críticos de la IA, también sugieren una arquitectura de red neuronal con una gran "biblioteca de casos", aunque identifican varios problemas, como la necesidad de conocimiento previo y las dificultades con redes neuronales que trabajan con muchas características.

Muchos de estos problemas han sido abordados con éxito, como el aprendizaje supervisado y no supervisado, el uso de grandes conjuntos de datos, y sistemas que pueden generar nuevas características. Además, los sistemas de visión activa permiten que los robots procesen información de manera más eficiente, como en el caso del robot STANLEY que cruzó un desierto.

Finalmente, Dreyfus argumenta a favor de agentes situados, es decir, aquellos que interactúan con su entorno, en lugar de simples motores de inferencia lógica desincorporados. La "cognición encarnada" sostiene que la mente no puede entenderse sin considerar el cuerpo y el entorno, siendo estos elementos fundamentales para la cognición, especialmente en la robótica y la visión.

## Strong IA: CAN MACHINES REALLY THINK?

La inteligencia artificial fuerte es la afirmación de que las computadoras pueden realmente pensar y no solo simular la inteligencia. Muchos filósofos han afirmado que una máquina que pasara el Test de Turing todavía no estaría realmente pensando, sino que sería sólo una simulación del pensamiento. 

"Hasta que una máquina no pueda escribir un soneto o componer un concierto a partir de los pensamientos y emociones que siente, y no por la caída fortuita de los símbolos, no podríamos estar de acuerdo en que la máquina es igual al cerebro, es decir, que no sólo lo escribe, sino que sabe que lo ha escrito. Turing llama a esto el argumento de la conciencia: la máquina tiene que ser consciente de sus propios estados mentales y acciones.

Jefferson se relaciona con la fenomenología, o el estudio de la experiencia directa: la máquina tiene que sentir realmente emociones. Otros se centran en la intencionalidad, es decir, la cuestión de si las supuestas creencias, deseos y otras representaciones de la máquina son realmente "sobre" alguna cosa en el mundo real.

La respuesta de Turing ante estos y otros argumentos, la convención cortés, sugiere que el problema eventualmente desaparecerá por sí solo una vez que las máquinas alcancen un cierto nivel de sofisticación. Esto tendría el efecto de disolver la diferencia entre una IA débil y una fuerte.

Los filósofos fisicalistas han intentado explicar lo que significa decir que una persona —y, por extensión, una computadora— se encuentra en un estado mental particular.

El concepto de "cerebro en una cubeta" explora cómo los estados mentales (como creer, saber o desear) están relacionados con los estados cerebrales. Los filósofos fisicalistas argumentan que un estado mental es determinado por el estado físico del cerebro. Por ejemplo, si estás comiendo una hamburguesa, tu estado cerebral corresponde a "saber que estás comiendo una hamburguesa". Sin embargo, el experimento mental del cerebro en una cubeta desafía esta idea. Imagina que tu cerebro está en una cubeta conectada a una simulación que te hace creer que comes una hamburguesa. Aunque tu estado cerebral es idéntico al de alguien que realmente come una hamburguesa, no estás comiendo una. Esto plantea dudas sobre si los estados cerebrales determinan completamente los estados mentales.

Para resolver este dilema, se proponen dos puntos de vista: contenido amplio y contenido estrecho. El contenido amplio incluye tanto el estado del cerebro como el entorno, mientras que el contenido estrecho solo se enfoca en el estado cerebral. El contenido amplio es útil para interpretar el comportamiento de otros en el mundo real, pero para preguntas sobre si una IA realmente piensa, el contenido estrecho es más adecuado, ya que se centra en el funcionamiento interno del sistema.

**Funcionalismo:**

El funcionalismo sostiene que un estado mental es una condición causal intermedia entre una entrada y una salida. Según esta teoría, dos sistemas con procesos causales isomorfos tendrían los mismos estados mentales, lo que implica que un programa de computadora podría tener los mismos estados mentales que una persona.

El experimento mental del reemplazo cerebral, propuesto por Moravec, plantea que si reemplazamos gradualmente todas las neuronas de una persona por dispositivos electrónicos que replican su comportamiento, el individuo conservaría su conciencia y comportamiento externo. Sin embargo, Searle argumenta que la conciencia desaparecería, aunque el comportamiento externo permanezca intacto, como si la persona respondiera automáticamente sin estar consciente.

Hay tres posibles conclusiones: 1) el cerebro electrónico es consciente, 2) la conciencia no influye en el comportamiento y es epifenoménica, o 3) el experimento es imposible. La segunda opción reduce la conciencia a un fenómeno sin efectos en el mundo observable. Además, Churchland señala que si aceptamos que el cerebro electrónico es consciente, también deberíamos aceptar que un sistema que usa una tabla de búsqueda para generar respuestas podría ser consciente, lo cual inquieta a muchos, ya que parece que este tipo de sistemas no deberían generar experiencias conscientes de la misma manera.

**El naturalismo biológico y la habitación china:**

John Searle, con su teoría del naturalismo biológico, desafía el funcionalismo al sostener que los estados mentales son características emergentes causadas por procesos físicos en las neuronas. Según Searle, los estados mentales no pueden replicarse simplemente con un programa que tenga la misma estructura funcional, ya que se requiere una arquitectura con el mismo poder causal que las neuronas.

Para ilustrar su argumento, Searle presenta el "Experimento de la Habitación China", donde una persona sigue instrucciones sin entender chino, pero desde el exterior parece que el sistema sí lo entiende. Esto muestra que, aunque el programa genere las respuestas correctas, no implica comprensión.

Searle propone cuatro axiomas, destacando que los programas son sintácticos, mientras que las mentes tienen contenido semántico, y concluye que los programas no son suficientes para crear una mente. A pesar de las respuestas críticas, Searle insiste en que para crear una mente, se necesita algo equivalente al poder causal de las neuronas, aunque lo que estas propiedades son no está claro.

**Conciencia, qualia y la brecha explicativa:**

El tema de la conciencia es fundamental en los debates sobre la IA fuerte, particularmente la experiencia subjetiva o "qualia". Esta se refiere a por qué ciertas experiencias cerebrales, como comer una hamburguesa, generan sensaciones subjetivas, mientras que otros estados físicos, como ser una roca, no lo hacen.

Un desafío para el funcionalismo es que diferentes qualia podrían estar asociadas con procesos causales isomorfos, como en el experimento mental del espectro invertido, donde una persona ve los colores de forma diferente pero sigue comportándose igual.

La ciencia actual no puede explicar cómo los procesos neuronales generan experiencias subjetivas, lo que ha llevado a algunos filósofos a pensar que somos incapaces de comprender nuestra propia conciencia. Otros, como Daniel Dennett, niegan la existencia de qualia, considerándolas una confusión filosófica.

Turing reconoció el misterio de la conciencia, pero pensaba que no es necesario resolverlo para avanzar en la creación de programas que se comporten de manera inteligente.

## La ética y los riesgos de desarrollar Inteligencia Artificial.

La IA parece plantear algunos problemas de ética qe afectan directamente a la humanidad: 
- Las personas podrían perder sus empleos debido a la automatización:

Las personas podrían perder sus empleos debido a la automatización. La economía industrial moderna se ha vuelto dependiente de las computadoras en general, y de ciertos programas de IA en particular. Se podría decir que miles de trabajadores han sido desplazados por estos programas de IA, pero en realidad si se quitaran los programas de IA estos puestos de trabajo no existirían, porque el trabajo humano añadiría un coste inaceptable a las transacciones. la automatización a través de la tecnología de la información en general y de la IA en particular ha creado más puestos de trabajo de los que ha eliminado, y ha creado puestos de trabajo más interesantes y mejor remunerados.

- Las personas pueden perder su sentido de ser únicas:

El contraargumento a ese problema es el siguiente: De Revolutionibus Orbium Coelestium (Copérnico, 1543) alejó a la Tierra del centro del sistema solar, y El origen del hombre (Darwin, 1871) puso al Homo sapiens al mismo nivel que otras especies. La IA, si tiene un gran éxito, puede ser al menos tan amenazante para los supuestos morales de la sociedad del siglo XXI como lo fue la teoría de la evolución de Darwin para los del siglo XIX.

- Los sistemas de IA pueden utilizarse con fines indeseables:

Los sistemas autónomos de IA son ahora habituales en el campo de batalla; el ejército estadounidense desplegó más de 5.000 aviones autónomos y 12.000 vehículos terrestres autónomos en Irak. Luego tambien existe la perdida de privacidad, utilizando la IA para realizar escuchas telefonicas, camaras con reconocimiento facial, entre otras. Este punto no tiene un contraargumento, dado a que muchos descubrimientos positivos (como la fisión nuclear) resultaron en objetos muy negativos como la bomba atómica.

- Otros problemas: El éxito de la IA podría significar el fin de la raza humana, la humanidad puede tener menos o mas tiempo libre, entre otros.

# Ejercicio 2: 

## 1. ¿Es posible considerar a los agentes conversacionales basados en grandes modelos de lenguaje (LLMs) como conscientes?

La cuestión de si los LLM "realmente" tienen creencias se convierte en un tema de debate filosófico. Ddebemos tener cuidado de no tomar demasiado en serio hablar de creencias en agentes conversacionales simples basados en LLM, a pesar de sus impresionantes habilidades conversacionales, ya que carecen de los medios para "participar plenamente en el juego de la verdad del lenguaje humano". Específicamente, un agente conversacional basado en LLM simple, de acuerdo con la definición de "simple" que se usa aquí, no puede medir sus palabras contra la realidad externa y actualizar lo que dice en consecuencia, una capacidad que es central para el concepto de creencia en su sentido más pleno.

Los objetivos e intenciones de dicho agente no reflejan sus propias necesidades o deseos porque no tiene ninguno, al menos no en un sentido literal (ni siquiera un deseo de ayudar al usuario). Si profesa tener, por ejemplo, un deseo de autoconservación, se trata de un "mero" juego de roles. No hay nada que pueda calificar como un ser digno de ser preservado para un agente de este tipo. No tiene cuerpo, ni historia personal, ni memoria autobiográfica.

## 2. ¿Cuáles son las implicaciones éticas de atribuir conciencia y, por ende, "derechos morales" a los agentes de IA avanzados?  

El resultado de atribuir conciencia y derechos moralos a los agentes de IA avanzados, creando un especie de metaverso y podría ser un mundo en el que las relaciones humanas auténticas se degraden más allá del reconocimiento, donde los usuarios prefieran la compañía de agentes de IA a la de otros humanos. O tal vez el mundo encuentre un camino intermedio y, existencialmente hablando, las cosas continúen más o menos como antes.

# Ejercicio 3: 
A partir de la lectura del artículo You Are Not a Parrot elaborar un breve comentario defendiendo el uso de la inteligencia artificial generativa a pesar de los comentarios observados en el artículo.

El primer argumento solido en contra de las LLMs es de donde estas obtienen información, la posible replicacion de informacion inmoral de las mismas y el metodo para censurar en estas el pensamiento que no es moralmente adecuado.
Este argumento tiene una gran validez, pero es tambien un argumento que se podría usar en contra de la misma sociedad. Un ejemplo de esto son los medios de comunicacion: Cuantos medios importantes han sido "amarillistas", no han chequeado su informacion, han impuesto su opinion como verdad o transgiversado la misma en busca de un beneficio. La diferencia es que las inteligencias artificiales solo tienen un cesgo y no se hace por un beneficio, ya que como se indico sus fuentes son provenientes de todo el internet, copias de libros, etc. 

Teniendo en cuenta el argumento de que las LLMs "imitan" a la perfeccion nuestro lenguaje, es un punto en el que, estoy personalmente de acuerdo con la autora y el cual encuentro importante revisar.
Sin embargo, esa imitación logra que el publico mas reacio o con menor capacidad de utilizar el internet para encontrar informacion importante, pueda acceder a esta, dado a que los LLMs la exponen mas facil de entender o de forma mas rapida.

